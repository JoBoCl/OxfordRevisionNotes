\documentclass[10pt]{article}
\title{Design and Analysis of Algorithms}
\author{}\date{}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[ruled,noline,noend,linesnumbered]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage[floats,charwidths,indent,lists,title,sections,margins]{savetrees}
\usepackage{mdwlist}
\usepackage{mathtools}
\usepackage{listings}
\newcommand{\hs}{\mathop{heapsize}}
\lstloadlanguages{Haskell}
\lstnewenvironment{code}
    {\lstset{}%
      \csname lst@SetFirstLabel\endcsname}
    {\csname lst@SaveFirstLabel\endcsname}
    \lstset{
      basicstyle=\small\ttfamily,
      flexiblecolumns=false,
      breaklines,
      basewidth={0.5em,0.45em},
      literate={+}{{$+$}}1 {/}{{$/$}}1 {*}{{$*$}}1 {=}{{$=$}}1
               {>}{{$>$}}1 {<}{{$<$}}1 {\\}{{$\lambda$}}1
               {\\\\}{{\char`\\\char`\\}}1
               {->}{{$\rightarrow$}}2 {>=}{{$\geq$}}2 {<-}{{$\leftarrow$}}2
               {<=}{{$\leq$}}2 {=>}{{$\Rightarrow$}}2 
               {\ .}{{$\circ$}}2 {\ .\ }{{$\circ$}}2
               {>>}{{>>}}2 {>>=}{{>>=}}2
               {|}{{$\mid$}}1               
    }
\usepackage{hyperref}
\begin{document}
\twocolumn
\maketitle
\tableofcontents
\listofalgorithms
\section{Analysis}
\subsection{Asymptotic Notation}
\subsubsection{Big-O Notation}
For the functions $f,g:\mathbb N\mapsto \mathbb R_{\ge0}$, let the set
\begin{align*}
	O(g(n)) :=\{&f:\mathbb N\mapsto \mathbb R^+ : \exists n_0\in\mathbb N^+ \cdot \exists c \in \mathbb R^+ \cdot\\
			  & n \ge n_0 \rightarrow f(n) \le c \times g(n)\}
\end{align*}
$f\in O(g)$, if, for some large enough $n$, $f(n)\le c\times g(n)$ for some constant factor $c$.  The set $O(g)$ is the set of functions bounded above by $g$, without taking into account constant factors.

When the equals sign is used in Big-O notation, it does not imply the usual property of symmetry associated with equality. $n=O(n^2)$, but $n^2\neq O(n)$.  Additionally, $n=O(n^3)$ and $n^2=O(n^3)$, but $n\neq n^2$.

Big-O notation has the following properties:
\begin{itemize}
	\item $\forall c \in R_{\ge0}$, if $f\in O(g)$ then $cf\in O(g)$
	\item $\forall c \in R_{\ge0}$, if $f\in O(g)$ then $f\in O(cg)$, (hence where writing log operations without bases comes from).
	\item If $f_1\in O\left( g_1 \right)$ and $f_2\in O\left( g_2 \right)$ then $f_1+f_2\in O\left( g_1+g_2 \right)$
	\item If $f_1\in O\left( g_1 \right)$ and $f_2\in O\left( g_2 \right)$ then $f_1+f_2\in O\left( \max\left(g_1,g_2\right) \right)$
	\item If $f_1\in O\left( g_1 \right)$ and $f_2\in O\left( g_2 \right)$ then $f_1\times f_2\in O\left( g_1\times g_2 \right)$
	\item If $f\in O\left( g \right)$, and $g\in O\left( h \right)$, then $f\in O(h)$
	\item Every polynomial of degree $l\ge 0$ is in $O\left( n^l \right)$
	\item $\forall c\in \mathbb R\cdot \lg\left( n^c \right)\in O\left( \lg n \right)$
	\item $\forall c,d>0 \cdot \lg^cn\in O\left( n^d \right)$
	\item $\forall c >0\land d>0 \cdot n^c\in O\left( d^n \right)$
\end{itemize}
\subsubsection{Big-Omega Notation}
For the functions $f,g:\mathbb N\mapsto \mathbb R_{\ge0}$, let the set
\begin{align*}
	\Omega(g(n)) :=\{&f:\mathbb N\mapsto \mathbb R^+ : \exists n_0\in\mathbb N^+ \cdot \exists c \in \mathbb R^+ \cdot\\
			  & n \ge n_0 \rightarrow f(n) \ge c \times g(n)\}
\end{align*}
$f\in \Omega(g)$, if, for some large enough $n$, $f(n)\ge c\times g(n)$ for some constant factor $c$.  The set $O(g)$ is the set of functions bounded below by $g$, without taking into account constant factors.
\subsubsection{Big-Theta Notation}
For the functions $f,g:\mathbb N\mapsto \mathbb R_{\ge0}$, if $f\in \Theta(g)$, then $g$ is an asymptotic upper and lower bound to $f$.  That is, for $c_1,c_2\in\mathbb R_{>0}$ and $n_0$ such that, for all $n\ge n_0$ \[c_1g(n)\le f(n)\le c_2g(n)\]
\subsection{Recurrence Relations}
There are four methods to solve recurrence relations:
\begin{enumerate}
	\item Recursion tree -- by unfolding at each level, and substituting in the non-recursive term, and approximation for each level can be found.  By determining the number of levels, and the cost of the final level, a total cost can be discovered.
	\item Iterative substitution -- similar to above, but without drawing the diagram.
	\item Guess-and-Test -- by substituting in an approximation, either a contradiction will be discovered $-T(n)\ge 0$, or none will be found, hence the correct form of $T(n)$ has been discovered.
	\item The Master Theorem
\end{enumerate}
\subsubsection{The Master Theorem}
Suppose\[T(n)=\begin{cases} O(1)&\text{ if }n=1\\ aT(\lceil n/b\rceil)+O(n^d)&\text{ if }n>1\end{cases}\]then for some constants $a>0,b>1,d\ge 0$
	\[T(n)=\begin{cases} O(n^d)&\text{ if }d>\log_ba\\O(n^d\log_bn)&\text{ if }d=\log_ba\\O(n^{\log_ba})&\text{ if }d<\log_ba\end{cases}\]
\subsection{Correctness and Invariants}
Loop-Invariant correctness proofs allow a program to be proven to be correct by a process similar to mathematical induction.
\begin{description}
	\item[Initialisation] -- prove that an invariant $I$ holds just before the first iteration (base case)
	\item[Maintenance] -- prove that if $I$ holds just before the start of a loop, it holds just before the next loop (inductive case)
	\item[Termination] -- prove that, when the loop terminates, $I$ and the reason for termination guarantee the end result is correct.
\end{description}
\section{Design Principles}
\subsection{Divide and Conquer}
The divide-and-conquer strategy can be thought of as solving problems in the following steps:
\begin{enumerate*}
\item Break the initial problem into subproblems.
\item Recursively solve the subproblems [if the problems are small enough, solve by brute force for a \emph{base case}].
\item Appropriately combine the answers to the subproblems.
\end{enumerate*}
The most complicate work is found in dividing the problems into the subproblems, at the tail-end of the recursion, when solving the subproblems, and gluing the intermediate answers together.
\subsubsection{Merge Sort}
\paragraph{Description}
This is a divide-and-conquer algorithm to sort the array $A\left[p..r\right)$.
\begin{enumerate*}
\item \emph{Divide} -- split the array into $A\left[p..q\right)$ and $A\left[q..r\right)$, where $q=\lfloor(p+q)/2\rfloor$.
\item \emph{Conquer} -- by recursively sorting the subarrays, and bottoming out the recursion when singleton arrays are reached.
\item \emph{Combine} -- by merging the sorted subarrays $A\left[p..q\right)$ and $A\left[q..r\right)$ using a $\Theta(n)$ procedure.
\end{enumerate*}
\paragraph{Pseudo-code}
\begin{algorithm}[!htp]
\caption{Merge Sort}
\SetAlFnt{\small}
\SetKwProg{Fn}{def}{}{}
\SetKwFunction{MERGESORT}{MERGE-SORT}
\SetKwFunction{MERGE}{MERGE}
\Fn{\MERGESORT$(A,p,r)$}{
	\KwIn{An integer array $A$ with indices $p<r$}
	\KwOut{The subarray $A[p..r)$ sorted in increasing order}
	\If{$p+1<r$}{
	$q=\lfloor(p+r)/2\rfloor$\;
	\MERGESORT$(A,p,q)$\;
	\MERGESORT$(A,q,r)$\;
	\MERGE$(A,p,q,r)$\;}
}
\setcounter{AlgoLine}{0}
\BlankLine
\Fn{\MERGE$(A,p,q,r)$}{
	\KwIn{Array $A$ with indices $p,q,r$ such that $p<q<r$ and subarrays $A[p..q)$ and $A[q..r)$ already sorted.}
	\KwOut{The subarrays are merged into a sorted array $A[p..r)$}
	$n_1=q-p$, $n_2=r-q$\;
	Create array $L$ of size $n_1+1$, Create array $R$ of size $n_2+1$\;
	\lFor{$i=0$ \KwTo$n_1-1$}{$L[i]=A[p+i]$}
	\lFor{$j=0$ \KwTo$n_2-1$}{$R[j]=A[q+j]$}
	$L[n_1]=\infty$, $R[n_2]=\infty$\;
	$i=0$, $j=0$\;
	\For{$k=p$ \KwTo$r-1$}{
		\leIf{$L[i]\le R[j]$\tcc{To ensure stable sort}}{$A[k]=L[i]$;$i=i+1$\;}{$A[k]=R[j]$;$j=j+1$}
		}
		\KwRet{A}\;
	}
\end{algorithm}
\texttt{MERGE} runs in $\Theta(n)$ as each of the \texttt{for} loops' lines run in constant time, iterating over $n$ elements, therefore the entire function is bounded by $n$.
\paragraph{Advantages}
\begin{itemize*}
\item Merge sort is stable, as the \texttt{MERGE} function is left biased.
\item Runtime is always $O(n\log n)$.
\end{itemize*}
\paragraph{Disadvantages}
\begin{itemize*}
\item Merge, and by extension, merge sort requires $O(n)$ extra space.
\item Merge sort is not an online algorithm.
\end{itemize*}
\paragraph{Haskell Implementation}
By pattern matching, first against the base cases, the program then recurses into two more calls of mergesort using two separate halves of the list.  The call to merge then ensure that the two halves of the list are joined into a correctly sorted list.
\begin{code}
mergesort :: Ord a => [a] -> [a]
mergesort [] = []
mergesort [x] = [x]
mergesort xs = merge (mergesort xs1) (mergesort xs2)
	where (xs1,xs2) = split xs

split :: [a] -> ([a],[a])
split (x:y:zs) = (x:xs,y:ys) where (xs,ys) = split zs
split xs = (xs,[])

merge :: [a] -> [a] -> [a]
merge xs [] = xs
merge [] ys = ys
merge (x:xs) (y:ys) = 
  case x <= y of
	True -> x : merge xs (y:ys)
	False -> y : merge (x:xs) ys
\end{code}
\subsubsection{Binary Search}
\paragraph{Description and Requirements}
\begin{itemize*}
\item The array to work with is sorted
\item Begin by looking at the middle element.  If the item to be found is less than the middle element, discard the latter half of the list, else discard the first half.
\item Recursively look for the element until a singleton is reached.  If this is the element, then return true/index of element, else return false/sensible error variable.
\end{itemize*}
\begin{algorithm}
\caption{Binary Search}
\KwIn{Array $A$ of distinct increasing integers, and an integer $z$}
\KwOut{``Yes'' if $z\in A$, ``No'' otherwise}
\SetKwProg{Fn}{def}{}{}
\SetKwFunction{BINS}{BIN-SEARCH}
\Fn{\BINS$(A,p,r,z)$}{
\eIf{$p\ge r$}{\KwRet{``No''}}
{
	$q=\lfloor(p+q)/2\rfloor$\;
	\eIf{$z=A[q]$}{\KwRet{``Yes''}}{\eIf{$z<A[q]$}{\BINS{$A,p,q,z$}\;}{\BINS{$A,q+1,r,z$}\;}}
	}
}
\end{algorithm}
\paragraph{Running time}Let $T(n)$ be the worst-case time, therefore \[T(n)=\begin{cases}O(1)&\text{ if }n=1\\T\left(\lfloor n/2\rfloor\right)+O(1)&\text{otherwise}\end{cases}\] By the Master Theorem, $T(n)=O(\log n)$.  By looking at the ``decision tree'' of the binary search, it is clear that binary search is $\Omega(\log n)$, therefore, $T(n)\in\Theta(\log n)$.
\subsubsection{Selection Problem}
The $i^\text{th}$ ordered statistic of a set of $n$ distinct elements is the element that is larger than exactly $i-1$ other elements.

Upper-bound time: $O(n\log n)$ -- sort the array in $O(n\log n)$ time and return the $i^\text{th}$ element.

It is possible to do so in worst case $O(n)$ time.

By partitioning the array, it is possible to recursively solve the above problem.
\begin{algorithm}
	\caption{Partition algorithm}
	\KwIn{An array $A$ of distinct numbers, with indices $p\le q<r$ and $m=A[q]$ as the pivot}
	\KwOut{An index $q'$ with $p\le q'\le r$ such that $A[p..r)$ is a permutation of $A$, $\forall a \in A[p..q') \Rightarrow a<m\,\wedge\,\forall a\in A[q'..r)\Rightarrow a\ge m\,\wedge\,A[q']=m$.}
\end{algorithm}
\begin{algorithm}
	\caption{Selection Algorithm}
	\SetKwFunction{SEL}{SELECT}
	\KwIn{An array $A$ of distinct numbers and the $i^\text{th}$ order statistic to find}
	\KwOut{The $i^\text{th}$ smallest element}
	Divide the $n$ input elements into groups of 5, with one group of $n\mod 5$ elements\;
	Find the median of the first $\lfloor n/5\rfloor$ groups in $O(1)$\;
	Find the median-of-medians $x$ by calling select recursively\;
	Partition the input array around $x$, with the lower partition having $k-1$ elements\;
	\lIf{$i=k$}{\KwRet{$x$}}
	\Else{
	\lIf{$i<k$}{\SEL{$A[0..k),i$}}\lElse{\SEL{$A[k+1..n),i-k$}}
	}
\end{algorithm}

Steps 1,2, and 4 take $O(n)$ time.

The number of elements $\le x$ is, at least $3\left(\lceil\frac12\lfloor\frac n5\rfloor\rceil\right)\ge\lceil\frac{3n}{10}\rceil-2$.
Thus, in the worst case, \texttt{SELECT} is called on, at most $\lfloor 7n/10\rfloor+2$ elements, so \[T(n)\le T(\lfloor n/5\rfloor)+T(\lfloor7n/10\rfloor+2)+cn\]for some constant $c\in\mathbb R$.

By supposing that $T(n)\in O(n)$, and then substituting $bn$ in for $T(n)$, for $b\ge 10cn/(n-20)$, $T(n)\le cn$ else $T(n)\in O(1)$.
\subsubsection{Integer Multiplication}
By using an observation of Gauss', that is $(a+bi)(c+di)=ac-bd+(bc+ad)i$ can be done using just three multiplication operations ($ac$, $bd$, and $(a+b)(c+d)$), integer multiplication can then be performed much more efficiently.  By splitting $n$-bit numbers $x$ and $y$ into left and right halves, each $n/2$ bits long, it is therefore possible to compute 
\begin{align*}
	xy&=\left(2^{n/2}x_L+x_R\right)\left(2^{n/2}y_L+y_R\right)\\
	&=2^nx_Ly_L+2^{n/2}\left(x_Ly_R+x_Ry_L\right) + x_Ry_R
\end{align*}
Multiplication by $2^n$ can be implemented as a left-shift in constant time, and addition in linear time, giving the time $T(n)$ to multiply two $n$-bit numbers as \[T(n)=4T(n/2)+O(n)\]which, by the Master Theorem, gives $T(n)\in O(n^2)$.

By using Gauss' trick, and re-writing as $x_Ly_L$, $x_Ry_R$ and $(x_L+x_R)(y_L+y_R)$, the running time becomes \[T(n)=3T(n/2)+O(n)\]Again, by the Master Theorem, $T(n)\in\left(n^{\log_23}\right)\approx O\left(n^{1.59\ldots}\right)$.
\subsubsection{Matrix Multiplication}
Multiplying the $p\times q$ matrix $X$ by the $q\times r$ matrix $Y$ gives \[Z_{ij}=\sum_{k=1}^qX_{ik}\cdot Y_{kj}\]The above requires $p\times q\times r$ multiplications and $p\times(q-1)\times r$ additions.  When $p=q=r=n$, there are $2n^3-n^2\in O(n^3)$ operations.  

By dividing $X$ and $Y$ into quarters, \[XY=\left[\begin{matrix}A&B\\C&D\end{matrix}\right]\left[\begin{matrix}E&F\\G&H\end{matrix}\right]=\left[\begin{matrix}AE+BG&AF+BH\\CE+DG&CF+DH\end{matrix}\right]\]It is then possible to recursively multiply the smaller matrices, and then add the individual elements in $O(n^2)$ time, giving a total running time of $T(n)=8T(n/2)+O(n^2)$, which, by the Master Theorem, is $O(n^3)$.

By observing (similarly to Gauss) that \[XY=\left[\begin{matrix}P_5+P_4-P_2+P_6&P_1+P_2\\P_3+P_4&P_1+P_5-P_3-P_7\end{matrix}\right]\]where
	\begin{align*}
		P_1&=A(F-H)&P_5&=(A+D)(E+H)\\
		P_2&=(A+B)H&P_6&=(B-D(G+H)\\
		P_3&=(C+D)E&P_7&=(A-C)(E+F)\\
		P_4&=D(G-E)&&
	\end{align*}
	Thus giving seven multiplications, hence $T(n)=7T(n/2)+O(n^2)\Rightarrow T(n)\in O\left(n^{\log_27}\right)\approx O\left(n^{2.81}\right)$.
\subsection{Dynamic Programming}
By identifying and solving a collection of smaller subproblems, and initial problem can be solved by using the solution to the smaller problems in building up a complete solutions.  Dynamic programming is not suited to solving all problems, as the solutions to the subproblems are not necessarily \emph{compositional}, i.e. the optimal solution is not formed from a composite of the solutions of the subproblems.

Dynamic programming is only applicable when the \emph{Principle of Optimality} is satisfied: ``The optimal solution to a problem is a composite of optimal solutions to (some of) its subproblems''.
\subsubsection{Knapsack Problems with Repetition}
For the $n$ items, each of weight $w_1,w_2\ldots w_n$ and value $v_1,v_2, \ldots v_n$ respectively, and a maximum carrying capacity of $W$ find the maximum possible value to carry without the total weight exceeding $W$.
Define $K[w]$ to be the maximum achievable value with weight limit $w$.  The solution to the problem is therefore \[K[w] = \max\left\{K\left[w-w_i\right]+v_i:w_i\le w\right\}\] where $K[W]$ is the desired answer.
\begin{algorithm}
\caption{Knapsack Problem with repetition}
\KwIn{List of weights $\{w_1,w_2,\ldots,w_n\}$ and respective values $\{v_1,v_2,\ldots,v_n\}$, and maximum weight $W$}
\KwOut{Maximum value of items}
$K[0]=0$\;
\lFor{$w=1$ \KwTo$W$}{$K[w] = \max\left\{K\left[w-w_i\right]+v_i:w_i\le w\right\}$}
\KwRet{$K[W]$}\;
\end{algorithm}

The above fills an one-dimensional array of length $W+1$, each entry taking $O(n)$ time, hence the total running time of the algorithm is $O(nW)$.
\subsubsection{Knapsack Problems without Repetition}
For the $n$ items, each of weight $w_1,w_2\ldots w_n$ and value $v_1,v_2, \ldots v_n$ respectively, and a maximum carrying capacity of $W$ find the maximum possible value to carry without the total weight exceeding $W$, and with all items being distinct.
Define $K[w,]$ to be the maximum achievable value with weight limit $w$, choosing from items $1,2\ldots j$, as $j$ varies between $0\le j \le n$.  The solution to the problem is therefore \[K[w,j] = \max\left\{K[w-w_j,j-1]+v_j, K[w,j-1]\right\}\]where $K[W,n]$ is the desired answer.
\begin{algorithm}
\caption{Knapsack Problem without repetition}
\KwIn{List of weights $\{w_1,w_2,\ldots,w_n\}$ and respective values $\{v_1,v_2,\ldots,v_n\}$, and maximum weight $W$}
\KwOut{Maximum value of items}
\lFor{$j=0$ \KwTo$n$}{ $K[0,j]=0$}
\lFor{$w=0$ \KwTo$W$}{ $K[w,0]=0$}
\For{$j=1$ \KwTo$n$}{ 
	\For{$w=1$ \KwTo$W$}{
		\eIf{$w_j>w$}{$K[w,j]=K[w,j-1]$\;}{$K[w] = \max\left\{K\left[w-w_i\right]+v_i:w_i\le w\right\}\;$}
	}
}
\KwRet{$K[W,n]$}\;
\end{algorithm}

The above fills an one-dimensional array of length $W+1$, each entry taking $O(n)$ time, hence the total running time of the algorithm is $O(nW)$.
\begin{code}
knapsack_spec :: [(Integer,Integer)] -> Integer -> Integer
knapsack_spec wvs w = 
      maximum ( map (sum . map snd) (
      filter ((<= w) .sum .map fst) (
      subsequences wvs )))

subsequences :: [a] -> [[a]] 
subsequences = foldr f [[]] 
  where f x = foldr (\y zs -> (x:y):y:zs) []

knapsack_rec [] w = 0
knapsack_rec ((wi,vi):wvs) w
  | wi > w = knapsack_rec wvs w
  | otherwise = max (knapsack_rec wvs w) (knapsack_rec wvs (w-wi) + vi)

knapsack_dp wvs wtot = table ! (wtot,n)
  where n = length wvs
        table = array ((0,0),(wtot,n)) [((w,j), ks w j) | w <- [0..wtot], j <- [0..n]]
        ks w 0 = 0
        kw w j = if wj > w then table ! (w,j-1)
                 else max (table ! (w,j-1))
                   ((table ! (w-wj, j-1)) + vj)
          where (wj,vj) = wvs !! (j-1)
\end{code}
\subsubsection{Change Making Problem}
Assuming an unlimited supply of coins, what is the minimum number of coins needed to give change to value $v$ using denominations $1=x_1,x_2,\ldots,x_n$?

By setting $C[u]$ to be the minimum number of coins required to give change to a total value of $u$, and looking for $C[v]$, the following recurrence is constructed:
\begin{align*}
C[0] &=0\\
C[u] &= 1+\min\left\{C[u-x_1]:1\le i\le n\,\wedge\, u\ge x_i\right\}
\end{align*}
The above fills an one-dimensional array of length $v$ with each entry taking, at most $O(n)$ time, hence the total running time is $O(nv)$.
\begin{algorithm}
\caption{Change giving algorithm}
\KwIn{List of coin denominations $1=x_1,x_2,\ldots,x_n$, and value of change $v$}
\KwOut{Minimum number of coins required to give change}
$C[0]=0$\;
\For{$u = 1$ \KwTo$v$}{
$C[u] = 1+\min\left\{C[u-x_1]:1\le i\le n\,\wedge\, u\ge x_i\right\}$\;
}
\KwRet{$C[v]$}\;
\end{algorithm}
\subsubsection{Edit Distance Problem (Levenshtein Distance)}
The edit distance of a string is the ``minimum number of edits needed to transform one string into another'' where an edit is an insertion, deletion or substitution.

For the strings $x[0..m)$ and $y[0..n)$, let $0\le i\le m$ and $0\le j \le n$, set $E[i,j]$ to be the edit distance between $x[0..i)$ and $y[0..j)$, and find $E[m,n]$.

There are three cases to be considered:
\begin{enumerate*}
\item Cost$=1$, to align $x[0..i-1)$ with $y[0..j)$ (insertion)
\item Cost$=1$, to align $x[0..i)$ with $y[0..j-1)$ (deletion)
\item Cost$=1\text{ if }x[i]\neq y[j]\text{ and }0\text{ otherwise}$, to align $x[0..i-1)$ with $y[0..j-1)$.
\end{enumerate*}
By letting $\delta(i,j):=1\text{ if }x[i]\neq y[j]\text{ and }0\text{ otherwise}$, then
\[E[i,j]=\min\{E[i-1,j]+1, E[i,j-1]+1,E[i-1,j-1]+\delta(i,j)\}\]
\begin{algorithm}
\caption{Levenshtein Distance}
\KwIn{Strings $x[0..m)$ and $y[0..n)$}
\KwOut{Edit distance between $x$ and $y$}
\lFor{$i=0$ \KwTo$m$}{$E[0,i]=i$}
\lFor{$j=0$ \KwTo$n$}{$E[j,0]=j$}
\For{$i=1$ \KwTo$m$}{
	\For{$j=1$ \KwTo$n$}{
$E[i,j]=\min\{E[i-1,j]+1, E[i,j-1]+1,E[i-1,j-1]+\delta(i,j)\}$
	}
}
\KwRet{$E[m,n]$}\;
\end{algorithm}
\subsubsection{Travelling Salesman Problem}
For the complete undirected graph with vertex-set $\{0,1,\ldots,n-1\}$ and edge lengths stored in the matrix $D=\left(d_{ij}\right)$.
Find a tour starting and ending at a specified node with minimum total length including all other vertices exactly once.
This problem is \emph{NP-hard}, as it is unlikely to ever be solved in polynomial time.  A brute force technique of examining every path takes $O(n!)$ ($(n-1)!$ possibilities), but dynamic programming reduces this to $O(n^22^n)$.

By considering the subset $\{0,1,\ldots,j\}\subseteq S\subseteq \{0,1,\ldots,n-1\}$, let $C[S,j]$ but the shortest simple path length starting at 0 and ending at $j$, visiting each node in $S$ exactly once.  For $|S|>1$, set $C[S,0]=\infty$ (simple graph, therefore cannot start and end at same node).  By expressing $S$ in terms of its subproblems: \[C[S,j]=\min\left\{C[S\setminus\{j\},i]+d_{ij}\mid i\in S\,\wedge\, i\neq j\right\}\]
The required answer is therefore \[\min\left\{C\left[\left\{O,1,\ldots,n-1\right\},j\right]+d_{j0}\mid 0\le j < n\right\}\]
There are, at most $n\cdot 2^n$ subproblems, each taking linear time to solve, giving a total running time of $O\left(n^22^n\right)$.
\subsubsection{All-pairs shortest path}
\label{sec:fw}
Given a directed graph $(V,E)$ with weight (considered as distance) $w:E\mapsto \mathbb{R}^{\ge0}$, for each pair of vertices $u$ and $v$, find the shortest path from $u$ to $v$.

Suppose the vertex-set is $\{0,1,\ldots,n-1\}$ and let $d[i,j;k]=\,$length of shortest path from $i$ to $j$, all of whose intermediate nodes are taken from $[0..k)$.

Initially, \[d[i,j;0]=\begin{cases}w(i,j)&\text{ if }(i,j)\in E\\\infty&\text{ otherwise}\end{cases}\]
If we have $d[i,k;k]$ and $d[k,j;k]$ then the shortest path that from $i$ to $j$ that uses $k$, as well as other nodes, goes through $k$ once, assuming no negative cycles.  Hence, $k$ is used in a shortest path from $i$ to $j$ iff $d[i,k;k]+d[k,j;k]<d[i,j;k]$, hence $d[i,j;k+1]$ should be updated accordingly.
\begin{algorithm}
\caption{Floyd-Warshall Algorithm}
\KwIn{The directed graph $(V,E)$ with weight (considered as distance) $w:E\mapsto \mathbb{R}^{\ge0}$}
\KwOut{Shortest path between all pairs of nodes}
\For{$i=0$ \KwTo$|V|-1$}{
\lFor{$j=0$ \KwTo$|V|-1$}{$d[i,j;0]=\infty$}}
\lForAll{edge $(i,j)\in E$}{$d[i,j;0]=w(i,j)$}

\For{$k=0$ \KwTo$|V|-1$}{
\For{$i=0$ \KwTo$|V|-1$}{
\For{$j=0$ \KwTo$|V|-1$}{
$d[i,j;k+1]=\min\{d[i,k;k]+d[k,j;k],d[i,j;k]\}$\;}}}
\KwRet{$d$}\;
\end{algorithm}
The running time is $O\left(|V|^3\right)$.
\subsection{Greedy Algorithms}
Similar to dynamic programming algorithms, these are also used to solve optimisation problems, and work by only choosing the step with the most immediate benefit as the next step, without looking ahead, or reconsidering earlier decisions.  They have the advantage that they are often more simple to implement, and there is no need for large amounts of storage, as only one decision is taken at each stage, and that decision is never reconsidered.
\subsubsection{Change Making Algorithms}
\paragraph{The Greedy Approach}Start with no change, and at each stage, choose a coin of the largest denomination available that does not exceed the balance to be paid.

However, the above method does not work with all denominations of coins, and does not always yield the optimal solution.  For example, with $100,60,50,5,1$, to pay $110$, the greedy algorithm would give 3, ($100+5+5$), whereas $60+50$ is a more optimal solution.
\subsubsection{Minimum Spanning Tree Algorithm}
\begin{algorithm}
	\caption{Generic MST algorithm}
	$A=\emptyset$\;
	\While{$A$ is not a spanning tree}{
		find an edge $(u,v)$ that is safe for $A$\;
		$A=A\cup\left\{(u,v)\right\}$\;}
		\KwRet{$A$}
\end{algorithm}
\begin{description*}
	\item[Loop Invariant]: ``$A$ is safe, i.e. a subset of some MST''
	\item[Initialisation]: The invariant is trivially satisfied by $A=\emptyset$
	\item[Maintenance]: Since only safe edges are added, $A$ remains a subset of some MST.
	\item[Termination]: All edges added to $A$ are in an MST so $A$ must be a spanning tree that is also minimal.
\end{description*}
If $(S,V\setminus S)$ is a cut that respects $A$, and $(u,v)$ is a light edge crossing the cut, then $(u,v)$ is safe for $A$.
\subsubsection{Kruskal's Algorithm}
\paragraph{Description}\begin{itemize*}
	\item Start with each vertex being its own connected component.
	\item Find the edge with the lowest weight.
	\item Merge two components by choosing the light edge connecting them
\end{itemize*}
Kruskal's requires a disjoint-set data structure to be most effective.  It is a set of disjoint sets $\mathcal S=\{S_1,\ldots,S_k\}$ where each set is represented by an individual element in each set.
\begin{algorithm}
	\caption{Kruskal's Algorithm}
	\KwIn{The graph $(V,E)$ and weight function $w:E\mapsto\mathbb R$}
	\KwOut{An MST for the graph}
	\SetKwFunction{MS}{MAKE-SET}
	\SetKwFunction{FS}{FIND-SET}
	\SetKwFunction{UN}{UNION}
	$A=\emptyset$\;
	\lForAll{$v\in V$}{\MS{$v$}}
	Sort $E$ by increasing weight $w$\;
	\ForAll{edges $(u,v)$ from the sorted list}{
	\If{\FS{$u$}$\neq$\FS{$v$}}{
		$A=A\cup\{(u,v)\}$\;
		\UN{$u,v$}\;
	}
	}
	\KwRet{$A$}\;
\end{algorithm}
\begin{description*}
	\item[Invariant]Let $\mathcal S$ be the collection of sets in the disjoint-set data structure and $L$ be the sorted list of edges not yet processed by the for-loop.
		\begin{enumerate*}
			\item$A$ is safe
			\item For each $C\in\mathcal S$, $(C,A\upharpoonright C)$ is a spanning tree of the subgraph $(C,E\upharpoonright C)$.
			\item Every processed edge's start- and end-points are in the same set in $\mathcal S$.
		\end{enumerate*}
	\item[Initialisation]$A=\emptyset$, $\mathcal S$ consists of only singleton sets and no edge has been processed $E\setminus L=\emptyset$ (hence all trivially true).
	\item[Maintenance]Let $e=(u,v)$ be the edge to be processed, $C_1$ and $C_2$ be the sets that $u$ and $v$ belong to respectively, and $A$, $\mathcal S$ and $L$ refer to the state before the iteration, and $A'$, $\mathcal S'$ and $L'$ be the state after the iteration.

		If $e$ is included in $A$, then $C_1$ and $C_2$ are different, and $e$ is the minimum edge crossing the cut $(C_1,V\setminus C_1)$, and the cut respects $A$.  As $e$ is the next element to be processed, it must also be the lightest element, hence the cut lemma holds.  The union operation ensures that the third part of the invariant holds.

		If $e$ is not to be included in $A$, then there are no changes to $A$, $\mathcal S$ and $L$ is updated to include the discarded edge.
	\item[Termination]All edges have been processed, therefore, $L=\emptyset$, and since all the nodes belong to the same set $C\in\mathcal S$, $C$ spans the whole graph, and by application of the cut lemma, and safe edges is an MST.
\end{description*}
\paragraph{Running Time}Initialisation of $A$ takes $O(1)$, first for-loop calls \texttt{MAKE-SET} $|V|$ times. $E$ is sorted in $|E|\log|E|$ time.  The second for-loop has $2|E|$ calls to \texttt{FIND-SET} and $|V|-1$ calls to \texttt{UNION}, giving a total running time of $O(|E|\log|E|)$.
\subsubsection{Prim's Algorithm}
\paragraph{Description}By growing the MST $A$ from a given root node $r$, at each stage, find a light edge crossing the cut $(V_A,V\setminus V_A)$ where $V_A$ is the edges incident on $A$.

The lightest edge can be found quickly by using a priority queue, where each entry in the queue is a vertex in $V\setminus V_A$.  $\mathop{key}[v]$ is the minimum weight of any edge $(u,v)$ where $v\in V_A$, the vertex returned by \texttt{EXTRACT-MIN} is $v$ such that $\exists u\in V_A$ where $(u,v)$ is a light edge crossing $(V_A,V\setminus V_A)$. $\mathop{key}[v]=\infty$ if $v$ is not adjacent to any vertex in $V_A$.
\begin{algorithm}
	\caption{Prim's Algorithm}
	\KwIn{The graph $(V,E)$ and weight function $w:E\mapsto\mathbb R$}
	\KwOut{An MST for the graph}
	\SetKwFunction{INSERT}{INSERT}
	\SetKwFunction{DK}{DECREASE-KEY}
	\SetKwFunction{EM}{EXTRACT-MIN}
	$Q=\emptyset$\;
	\ForAll{$u\in V$}{
	$\mathop{key}[u]=\infty$, $\pi[u]=\dagger$\;
	\INSERT{$Q,u$}\;
	}
	\DK{$Q,r,0$}
	\While{$Q\neq\emptyset$}{
		$u=\EM{Q}$\;
		\ForAll{$v\in\mathop{Adj}[u]$}{
			\If{$v\in Q\,\wedge\, w(u,v)<\mathop{key}[v]$}{
			$\pi[v]=u$, \DK{$Q,v,w(u,v)$}\;
			}
		}
	}
\end{algorithm}
\paragraph{Running Time}Initialising $Q$ takes $O(1)$, the first loop runs in $O(|V|)$, changing priority of $r$ takes $O(\log|V|)$, and $|V|$ \texttt{EXTRACT-MIN} calls are required with, at most $|E|$ \texttt{DECREASE-KEY} operations, giving running time of $O(|E|\log|V|)$.  The graph is connected, so $O(\log |E|)=O(\log|V|)$, hence total running time of $O(|E|\log |V|)$.
\subsection{Dynamic Programming vs. Divide-and-Conquer}
Dynamic programming is an optimisation technique, whereas divide-and-conquer is not normally used to solve optimality problems.

Both techniques split the input problems into smaller parts and use the solutions to the smaller parts to form a larger solution, however, dynamic programming solves the subproblems at all split points, whereas divide-and-conquer uses pre-determined split points using non-overlapping problems.  Dynamic programming uses solutions to already calculated subproblems to find the total solution, to reduce space complexity.
\section{Data Structures}
\subsection{Heaps}
A heap is a type of tree without explicit pointers.  Each level is filled from left to right, and the next level is only added when the previous is full.  All heaps satisfy either the max-heap or min-heap property: ``the value of a node (except the root node) is less than (greater than) or equal to that of its parent''.  In general, a heap can have any number of children on each of its nodes, and the maximum/minimum element of a max-/min-heap is at the root.  Heaps are used as efficient priority queues, and for heapsort, which has a complexity of $O(n\log n)$.
\subsubsection{Representation}
The root is always at $A[0]$, and for any node $i>0$, its parent is at $A[\lfloor(i-1)/2\rfloor]$ and its left and right children are at $A[2i+1]$ and $A[2i+2]$.
\subsubsection{Maintaining heaps}
\begin{algorithm}
	\caption{Heapify algorithm}
	\KwIn{Tree with left and right sub-trees of $i$ stored as heaps}
	\KwOut{$A$ where entire tree is also a heap.}
	\SetKwFunction{HEAPIFY}{HEAPIFY}
	$n=A.\hs$\;
	$l=2i+1$, $r=2i+2$\;
	\leIf{$l<n\,\wedge\,A[l]>A[i]$}{$\textit{largest}=l$}{$\textit{largest}=i$}
	\lIf{$r>n\,\wedge\,A[r]>A[\textit{largest}]$}{$\textit{largest}=r$}
	\If{$\textit{largest}\neq i$}{exchange $A[i]$ with $A[\textit{largest}]$\;\HEAPIFY{$A,\textit{largest}$}\;}
\end{algorithm}
\paragraph{Running Time}$\Theta(1)$ to find the largest of node and children.  Worst-case has tree with last row half full (i.e. subtree rooted at $i$ has, at most $2n/3$ elements), so $T(n)=T(2n/3)+\Theta(1)\Rightarrow T(n)=O(n^0\log_{3/2}n)=O(\log n)$ by the Master Theorem.
\begin{algorithm}
	\caption{Make-Heap algorithm}
	\KwIn{An unsorted integer array $A$ of size $n$}
	\KwOut{A heap of size $n$}
	\SetKwFunction{HEAPIFY}{HEAPIFY}
	$A.\hs=A.\mathop{length}$\;
	\lFor{$i=\lfloor{A.\mathop{length}/2\rfloor}$ \KwTo$0$}{\HEAPIFY{$A,i$}}
	\KwRet{$A$}
\end{algorithm}
\paragraph{Correctness}
\begin{description*}
	\item[Invariant]: each node $i+1,i+2,\ldots,n-1$ is the root of a heap for $-1\le i\le\lfloor n/2\rfloor$
	\item[Initialisation]: each node $\lfloor n/2\rfloor,\lfloor n/2\rfloor+1,\ldots,n-1$ is a leaf, which is the root of a trivial heap, therefore the invariant holds.
	\item[Maintenance]: calling \texttt{HEAPIFY}$(A,i)$ causes $i$ to become the root of a new heap, hence, when $i$ is decremented, nodes at $i+1,i+2,\ldots,n-1$ are all roots of heaps.
	\item[Termination]: when $i=-1$, the element at $0$ is the root of a heap, therefore all elements below it are also roots of heaps.
\end{description*}
\paragraph{Running Time}There are $n$ calls to \texttt{HEAPIFY}, each taking $O(\log n)$ time, giving $O(n\log n)$.

As \texttt{HEAPIFY} is linear with the height of the node that it runs on, the height of the heap is $\lfloor \lg n\rfloor$, hence the cost of \texttt{MAKE-HEAP} is
\[\sum_{h=0}^{\lfloor \lg n\rfloor}\left\lceil \frac{n}{2^{h+1}}\right\rceil O(h)=O\left(n\sum_{h=0}^{\lfloor \lg n\rfloor} \frac{h}{2^h}\right)\]As $\sum_{i=0}^\infty \frac i{2^i}=2$, the total running time is therefore $O(n)$.
\subsubsection{Heap-Sort}
\paragraph{Description}From a given input array, build a max-heap, and starting from the root element, swap the root node (maximum node) with the node at the end of the heap, and call \texttt{HEAPIFY} to maintain the heap property.  Repeat until the heap's size is 1, when the root node is the minimal element, and the array is therefore sorted.
\begin{algorithm}
	\caption{Heap-Sort}
	\KwIn{The unsorted array $A$ with distinct elements}
	\KwOut{Sorted permutation of $A$}
	\SetKwFunction{HFY}{HEAPIFY}
	\SetKwFunction{MH}{MAKE-HEAP}
	\MH{$A$}
	\For{$i=A.\hs-1$ \KwTo1}{
		swap $A[0]$ with $A[i]$\;
		$A.\hs = A.\hs-1$\;
		\HFY{$A,0$}\;
	}
	\KwRet{$A$}
\end{algorithm}
\paragraph{Running Time}\texttt{MAKE-HEAP} runs in $O(n)$, the \texttt{for}-loop is run $n-1$ times, with the swap operation, and decrementing the heap size taking $O(1)$.  \texttt{HEAPIFY} runs in $O(\log n)$ time, thus meaning the entire algorithm runs in $O(n\log n)$.
\paragraph{Advantages}Heap-sort is a stable sort that runs in $O(n\log n)$ time. In practice, however, a well implemented quicksort can beat heap-sort. 
\subsection{Priority Queues}
A priority queue is an abstract data structure, to maintain a sequence a values, each with an associated key or weight.

Priority queues normally have the following associated operations:
\begin{itemize*}
	\item \texttt{INSERT$(S,x,k)$} to insert the element $x$ with key $k$ into the queue $S$.
	\item \texttt{MAXIMUM$(S)$} to find the element in $S$ with the largest key. Minimum priority queues have an corresponding function \texttt{MINIMUM}.
	\item \texttt{EXTRACT-MAX$(S)$} to find and remove the element in $S$ with the largest key. Minimum priority queues have an corresponding function \texttt{EXTRACT-MIN}.
	\item \texttt{INCREASE-KEY$(S,x,k)$} to increase the value of $x$'s key to $k$.  The new value of $k$ must be larger than the old value. Minimum priority queues have an corresponding function \texttt{DECREASE-KEY}.
\end{itemize*}
Priority queues are often found used to schedule jobs, in Djikstra's shortest path algorithm and Prim's MST algorithm.
\subsubsection{Implementation}
It is possible to implement priority queues using an array or a doubly-linked list, however, the implementation of the extraction functions run in $\Theta(n)$ time, which can be improved with a heap implementation.

Implementing a priority queue using a heap allows the extraction operation to run in $O(\log n)$ time, however, it increases the time to insert an element from $O(1)$ to $O(\log n)$, but this is often seen as an acceptable trade-off.

The implementations provided all assume a maximum priority queue.
\begin{algorithm}
	\caption{Return the maximal element of the queue}
	\KwIn{The priority queue $A$}
	\KwOut{The maximum value of $A$}
	\KwRet{$A[0]$}\;
\end{algorithm}
\begin{algorithm}
	\caption{Extract Max Algorithm}
	\KwIn{The priority queue $A$}
	\SetKw{Er}{error}
	\KwOut{The old maximum value $A$}
	\SetKwFunction{HFY}{HEAPIFY}
	\lIf{$A.\hs<1$}{\Er{``heap underflow''}}
	$\textit{max} = A[0]$\;
	$A.\hs = A.\hs-1$\;
	$A[0]=A[A.\hs]$\;
	\HFY{$A,0$}\;
	\KwRet{$\textit{max}$}\;
\end{algorithm}
The running time is $O(\log n)$, as all calls, except the call to \texttt{HEAPIFY} run in $O(1)$ time.
\begin{algorithm}
	\caption{Increase Key Algorithm}
	\KwIn{The priority queue $A$, the value to increase $i$, and the new key $k$}
	\KwOut{The new priority queue $A$}
	\SetKw{Er}{error}
	\lIf{$k<A[i]$}{\Er{``new key is smaller than current key''}}
	$A[i]=k$\;
	\While{$i > 0\land A[i.\mathop{parent}]>A[i]$}{
		swap $A[i]$ and $A[i.\mathop{parent}]$\;
		$i=i.\mathop{parent}$\;
	}
	\KwRet{$A$}
\end{algorithm}
Running time: $O(\log n)$
\begin{algorithm}
	\caption{Insertion Algorithm}
	\KwIn{The priority queue $A$ and the key $k$}
	\KwOut{The new priority queue $A$}
	\SetKwFunction{HIK}{HEAP-INCREASE-KEY}
	$A.\hs = A.\hs+1$\;
	$A[A.\hs-1] = -\infty$\;
	\HIK{$A,A.\hs-1,k$}\;
	\KwRet{$A$}\;
\end{algorithm}
\subsection{Queues}
A FIFO queue is an abstract data structure, with the following methods:
\begin{description}
	\item[\texttt{ENQUEUE}$(Q,x)$] -- inserts $x$ at the end of the queue
	\item[\texttt{DEQUEUE}$(Q)$] -- finds and removes the element at the head of the queue $Q$.
	\item[\texttt{ISEMPTY}$(Q)$] -- tests if the queue $Q$ is empty.  Often abbreviated to $Q=\emptyset$ in pseudo-code.
\end{description}
\subsubsection{Implementation}
Linked list with an extra pointer to the tail of the list, with the head of the queue being the head of the linked list, making all operations $O(1)$.
\subsection{Graphs}
Graphs are often used to form abstractions to certain problems, for example, colouring a map to ensure two adjacent countries do not have the same colour is easier when the map is considered as a graph, where each country is a node, and two nodes are linked iff they share a border.
Graphs can also be used to help with exams scheduling: each exam is represented by a node, and joined if they are both taken by at least one student.
\subsubsection{Definitions}
\begin{description}
	\item[Directed graph]$(V,E)$, where $V$ is a set of nodes and $E\subseteq V\times V$ of edges.  If $u$ is connected to $v$ (does not imply $v$ is connected to $u$) by the edge $e=(u,v)$, we say that $e$ is incident on $u$ and $v$.
	\item[Path]from a vertex $u$ to $u'$ is a sequence $\langle v_0,v_1,\ldots,v_k\rangle$, with length $k$, of vertices such that $u=v_0$, $u'=v_k$ and each $(v_i,v_{i+1})\in E$.
	\item[Cycle]is a path such that $v_0=v_k$ and the path contains at least one edge.  Self-loops are cycles with length 1.  Simple cycles are cycles with all nodes being distinct.
	\item[Acyclic] graph are directed graphs containing no cycles.
	\item[Strongly connected] graphs are graphs that, for any two nodes, there is a path from the first to the second.
	\item[Undirected] graphs are graphs where $E$ is symmetric ($(u,v)\in E\Leftrightarrow (v,u)\in E$) 
\end{description}
\subsubsection{Representations}
Graphs can be represented as an adjacency matrix (where the entry $a_{ij}=1$ if $(v_i,v_j)\in E$ and $0$ otherwise.  An edge can be found in constant time, and requires a storage size of $O(|V|^2)$.  The undirected graph's representation is symmetric.

Adjacency lists consist of $|V|$ linked lists, with the list for vertex $u$ containing links to all vertices that $v$ is adjacent to.  The data structure cannot be checked in constant time, and has size $O(|V|+|E|)$.  Undirected graphs are represented by lists where $u$ is in $v$'s list iff $v$ is in $u$'s.
\subsubsection{Depth-first Search (DFS)}
Linear time algorithm to explore a graph by exploring all the reachable unseen vertices from each vertex.

As DFS progresses, each nodes is assigned a changing colour.
\begin{description}
	\item[Black]finished -- all reachable vertices have been discovered
	\item[Grey] discovered but not finished
	\item[White] not yet discovered
\end{description}
\begin{algorithm}
	\caption{Depth-First Search Algorithm}
	\KwIn{Graph $G=(V,E)$}
	\KwOut{Discovery and finishing time for each vertex $v\in V$}
	\SetKwProg{Fn}{def}{}{}
	\SetKwArray{Cl}{colour}
	\SetKwArray{Disc}{d}
	\SetKwArray{Fin}{f}
	\SetKwFunction{DFS}{DFS}
	\SetKwFunction{DFSV}{DFS-VISIT}
	\SetKwData{B}{BLACK}
	\SetKwData{G}{GREY}
	\SetKwData{W}{WHITE}
	\Fn{\DFS{$V,E$}}{
	\lForAll{$u\in V$}{\Cl{$u$}=\W}
	$t=0$\;
	\ForAll{$u\in V$}{
	\If{\Cl{$u$}$=$\W}{
			\DFSV{$u$}\;}
	}}
	\Fn{\DFSV{$u$}}{
	\Cl{$u$}$ = $\G\;
		$t = t+1$\;
		\Disc{$u$}$ = t$\;
		\ForAll{$v\in\mathop{Adj}[u]$}{
		\If{\Cl{$u$}$ = $\W}{
			\DFSV{$u$}\;}
			}
			\Cl{$u$}$ = $\B\;
			\Fin{$u$}$ = t$\;
	}
\end{algorithm}
\paragraph{Running Time} The first loop runs in $\Theta(|V|)$.  \texttt{DFS-VISIT} is called once for each $v\in V$, since it is not called on grey or black vertices.  During \texttt{DFS-VISIT}, the loop is run $|\mathop{Adj}[v]|$ times.  As $\sum_{v\in V}|\mathop{Adj}[v]|=\Theta(|E|)$, the total running cost of \texttt{DFS-VISIT}, as $v$ changes over $V$ is $\Theta(|E|)$, thus giving a total running time of $O(|V|+|E|)$ for depth-first search.
\paragraph{Depth-First Search Forest}
By analysing the arrays produced by DFS, a number of DFS trees can be produced, making up a DFS forest.  Each tree is made up of edges $(u,v)$ such that $u$ is grey and $v$ is white when $(u,v)$ is first explored. If a node $u$ is a descendent of $v$ in the DFS forest, it is also a descendent of $v$ in the original graph.

By using the following shorthand:\[\begin{matrix} d[u]&f[u]&d[v]&f[v]\\(_u&)_u&(_v&)_v\end{matrix}\]and the convention the value associated with the preceding bracket is less than the next.  The following rules are derived:
\begin{description}
	\item[$(_u\;)_u\;(_v\;)_v$]Neither $u$ nor $v$ are descendants of each other.
	\item[$(_u\;(_v\;)_v\;)_u$]$v$ is a descendent of $u$.
	\item[$(_u\;(_v\;)_u\;)_v$]cannot happen.
\end{description}
There are four different types of edge in a DFS forest:
\begin{description}
\item[Tree] -- edges of the DFS forest
\item[Back] -- lead from a node to an ancestor
\item[Forward] -- lead from a node to a non-child descendent
\item[Cross] -- can link from either same or different trees.  Always lead to edges with earlier discovery times, and only exist in directed graphs.
\end{description}
A directed graph has a cycle iff its DFS forest contains a back edge.
\subsubsection{Topological Sort}
The topological sort of a directed acyclic graph is a total order of the vertices such that if $(u,v)\in E$ then $u>v$.
\begin{algorithm}
	\caption{Topological Sort}
	\KwIn{A directed acyclic graph $G=(V,E)$}
	\KwOut{Elements of $V$ in topological order}
	\SetKwFunction{DFS}{DFS}
	\SetKwArray{Fin}{f}
	\SetKwArray{Dis}{d}
	Call \DFS{$V,E$} to find \Fin{$v$} for $v\in V$\;
	Output vertices in order of decreasing finishing times\;
\end{algorithm}
The running time of the above is $\Theta(|V|+|E|)$.
\subsubsection{Strongly Connected Components}
In directed graphs, two vertices $u$ and $v$ are strongly connected if there is a path, both from $u$ to $v$ and from $v$ to $u$.  A strongly connected component is the largest possible set of vertices that are all connected.

For the directed graph $G=(V,E)$, the transpose is $G^T=(V,E^T)$, where $E^T=\{(v,u)\mid(u,v)\in E\}$.  $G^T$ can be computed in $\Theta(|E|+|V|)$ using adjacency lists.  $G$ and $G^T$ both have the same strongly connected components, which can be used to identify the SCC of $G$.
\begin{algorithm}
	\caption{Strongly Connected Component Discovery}
	\KwIn{The directed graph $G$}
	\KwOut{Elements of each SCC in $G$ outputted in turn}
	\SetKwFunction{DFS}{DFS}
	\SetKwArray{Fin}{f}
	Call \DFS{$G$} to find \Fin{$u$} for all $u$\;
	Compute $G^T$\;
	Call \DFS{$G^T$}, but in the loop of \DFS, order by decreasing \Fin{$u$} as computed above\;
	Output the vertices in each tree of the DFS forest as found above\;
\end{algorithm}
\subsubsection{Breadth-First Search (BFS)}
Linear time algorithm to explore a graph by exploring all the reachable unseen vertices from each vertex.

By sending out a `wave' from a source edge $s$, all nodes 1 edge from $s$ will be discovered, then all nodes 2 edges away, etc.  A queue is used to maintain the wavefront, with the property that $v\in Q\Leftrightarrow$ wave has hit $v$, but has not yet moved past it.
\begin{algorithm}
	\caption{Bread-First Search}
	\KwIn{The graph $G=(V,E)$ and a source node $s\in V$}
	\KwOut{The discovery time and shortest distance from $s$ for each node $v$}
	\SetKwArray{Disc}{d}
	\SetKwArray{Path}{$\pi$}
	\SetKwData{null}{null}
	\SetKwFunction{EQ}{ENQUEUE}
	\SetKwFunction{DQ}{DEQUEUE}
	
	\Disc{$v$}$=0$, \Path{$v$}$=$\null\;
	\lForAll{$u\in V\setminus\{s\}$}{
		\Disc{$u$} = $\infty$, \Path{$u$} = \null
	}
	$Q=\emptyset$\;
	\EQ{$Q,s$}\;
	\While{$Q\neq\emptyset$}{
		$u=\;$\DQ{$Q$}\;
		\ForAll{$v\in\mathop{Adj}[u]$}{
			\If{\Disc$[v] = \infty$}{
				\Disc$[v] =\;$\Disc$[u]+1$\;
				\Path$[v]=u$\;
				\EQ{$Q,v$}\;
			}
		}
	}
\end{algorithm}
The initial setup records that $s$ can be reached from itself in 0 time and in 0 steps, and that, as far as we know, all other vertices cannot be reached, and therefore take an infinite amount of time to be reached.  In the \texttt{for}-loop, the next unfinished node is examined.  All adjacent nodes that have not yet been seen have their shortest distance recorded and their backpointer set.
\paragraph{Running Time}$O(|V|+|E|)$.  $O(|V|)$ as every vertex is enqueued at most once, $O(|E|)$ as every vertex dequeued at most once, and $(u,v)$ is examined only when $u$ is dequeued, therefore every edge is examined at most once if the graph is directed, and twice is the graph is undirected.

Unlike DFS, BFS may not reach all the vertices, as it only works outwards from a source vertex.  If there is no path from $s$ to $v$, then it will not be discovered.
\subsubsection{Shortest Path Problems}
Consider a directed graph $G=(V,E)$ with weight or length $w:E\mapsto \mathbb R$.

The weight of a path $p=\langle v_0,v_1,\ldots,v_k\rangle$ is given as \[w(p):=\sum_{i=1}^k w(v_{i-1},v_i)\]

The shortest-path weight from $u$ to $v$ is given as
\[\delta(u,v) := \begin{cases} \min\{ w(p) : p \text{ is a path from $u$ to $v$}\} &\exists\text{ path from $u$ to $v$}\\\infty&\text{otherwise}\end{cases}\]

A shortest path from $u$ to $v$ is a path such that $\delta(u,v)=w(p)$.

There are a number of different shortest path algorithms:
\begin{description}
	\item[Dynamic Programming] -- solves the single-source problem, for a DAG in $\Theta(|E|+|V|)$
	\item[BFS] -- solves the single-source problem for graphs with unit weight ($\forall e\in E\cdot w(e)=1$) in $O(|V|+|E|)$.
	\item[Djiksktra] -- solves the single-source problem for directed or undirected graphs with non-negative weights in $O\left( \left( |V|+|E| \right)\log |V| \right)$
	\item[Floyd-Warshall] -- solvers the all-pairs shortest path probelm for graphs without negative weight cycles in $O\left(|V|^3\right)$
\end{description}
\subsubsection{Djikstra's Algorithm}
Using a priority queue, Djikstra's adapts BFS to construct the shortest-path tree from $\pi[v]$.

By maintaining two sets of vertices -- $S$ for vertices where the shortest-path is known, and $Q=V\setminus S$ to be processed (stored in the priority queue).
\begin{algorithm}
	\caption{Djikstra's Shortest Path Algorithm}
	\KwIn{The graph $G=(V,E)$, $s\in V$ as the source node, $w:E\mapsto \mathbb R_{\ge 0}$}
	\KwOut{For each $v\in V$, $d[v]$ and $\pi[v]$}
	\SetKwArray{Disc}{d}
	\SetKwArray{Path}{$\pi$}
	\SetKwData{null}{null}
	\SetKwFunction{MQ}{MAKE-QUEUE}
	\SetKwFunction{EM}{EXTRACT-MIN}
	\SetKwFunction{DK}{DECREASE-KEY}
	
	\lForAll{$v\in V$}{\Disc{$v$}$\;=\infty$, \Path{$v$} = \null}
	\Disc{$s$}$\;=0; S=\emptyset$\;
	$Q=\;$\MQ{$V$} with \Disc{$v$} as keys\;
	\While{$Q\neq \emptyset$}{
		$u=\;$\EM{$Q$}\;
		$S=S\cup \{u\}$\;
		\ForAll{$v\in\mathop{Adj}[u]$}{
		\If{$\Disc[u]+w(u,v) <\Disc[v]$}{
			$\Disc[v] = \Disc[u] +w(u,v)$\;
			\Path{$v$}$\;=u$\;		
			\DK{$Q,v,$\Disc{$v$}}\;
			}
	}
	}
\end{algorithm}
To check for the presence of a unique shortest path, create an aray $u$, initialised to \texttt{true}.  By modifying the \texttt{if}-clause in the inner \texttt{for}-loop, if there is a new shortest path for a node $v$, then the existence of a new shortest path is dependent on the existence of a shortest path to its parent.  If a shortest path has already been discovered, and another is found, then $u[v]=\texttt{false}$.
\subsubsection{Floyd-Warshall} See section~\ref{sec:fw}
\subsubsection{Bellman-Ford}This is used to test for the presence of a negative weight cycle from a specific node.
\begin{algorithm}
	\caption{Bellman-Ford Algorithm}
	\KwIn{The graph $G=(V,E)$, $s\in V$ as a source node, $w:E\mapsto \mathbb R$}
	\KwOut{False if a negative weight cycle can be found from $s$, else $d[v]$, $\pi[v]$ for all $v\in V$}
	\SetKwArray{Disc}{d}
	\SetKwArray{Path}{$\pi$}
	\SetKwData{null}{null}

	\lForAll{$v\in V$}{$\Disc[v]=\infty;\Path[v]=\null$}
	$\Disc[s]=0$\;
	\For{$i=1\KwTo |V|-1$}{
		\ForAll{$(u,v)\in E$}{
			\lIf{$\Disc{u}+w(u,v)<\Disc{v}$}{ $\Disc{v}=\Disc{u}+w(u,v);\Path{v}=u$}
		}
	}
	\ForAll{$(u,v)\in E$}{
		\lIf{$\Disc[u] + w(u,v) < \Disc[v]$}{
			\KwRet{TRUE}
		}
	}
	\KwRet{FALSE}\;
\end{algorithm}
\paragraph{Running Time}The initialisation iterates over $V$, so takes $\Theta(|V|)$.  Each of the iterations of the second loop then iterate over the edges in $E$, giving a runtime of $O(|V|\Theta(|E|))$, and the final loop requires $O(|E|)$.  Therefore, the total runtime is $O(|V||E|)$.
\end{document}
